{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rkAz6t_8pJw1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting langchain==0.1.6\n",
            "  Using cached langchain-0.1.6-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in c:\\programdata\\anaconda3\\envs\\project\\lib\\site-packages (from langchain==0.1.6) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from langchain==0.1.6) (2.0.32)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from langchain==0.1.6) (3.10.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from langchain==0.1.6) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from langchain==0.1.6) (0.6.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from langchain==0.1.6) (1.33)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.18 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from langchain==0.1.6) (0.0.20)\n",
            "Requirement already satisfied: langchain-core<0.2,>=0.1.22 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from langchain==0.1.6) (0.1.23)\n",
            "Requirement already satisfied: langsmith<0.1,>=0.0.83 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from langchain==0.1.6) (0.0.87)\n",
            "Requirement already satisfied: numpy<2,>=1 in c:\\programdata\\anaconda3\\envs\\project\\lib\\site-packages (from langchain==0.1.6) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from langchain==0.1.6) (2.8.2)\n",
            "Requirement already satisfied: requests<3,>=2 in c:\\programdata\\anaconda3\\envs\\project\\lib\\site-packages (from langchain==0.1.6) (2.32.3)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\programdata\\anaconda3\\envs\\project\\lib\\site-packages (from langchain==0.1.6) (8.2.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.6) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.6) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\programdata\\anaconda3\\envs\\project\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.6) (23.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.6) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.6) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.6) (1.9.4)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.6) (3.22.0)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.6) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from jsonpatch<2.0,>=1.33->langchain==0.1.6) (3.0.0)\n",
            "Requirement already satisfied: anyio<5,>=3 in c:\\programdata\\anaconda3\\envs\\project\\lib\\site-packages (from langchain-core<0.2,>=0.1.22->langchain==0.1.6) (4.2.0)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from langchain-core<0.2,>=0.1.22->langchain==0.1.6) (23.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from pydantic<3,>=1->langchain==0.1.6) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from pydantic<3,>=1->langchain==0.1.6) (2.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\programdata\\anaconda3\\envs\\project\\lib\\site-packages (from pydantic<3,>=1->langchain==0.1.6) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\envs\\project\\lib\\site-packages (from requests<3,>=2->langchain==0.1.6) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\envs\\project\\lib\\site-packages (from requests<3,>=2->langchain==0.1.6) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\envs\\project\\lib\\site-packages (from requests<3,>=2->langchain==0.1.6) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\envs\\project\\lib\\site-packages (from requests<3,>=2->langchain==0.1.6) (2024.7.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from SQLAlchemy<3,>=1.4->langchain==0.1.6) (3.0.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in c:\\programdata\\anaconda3\\envs\\project\\lib\\site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.22->langchain==0.1.6) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\programdata\\anaconda3\\envs\\project\\lib\\site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.22->langchain==0.1.6) (1.2.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.1.6) (1.0.0)\n",
            "Using cached langchain-0.1.6-py3-none-any.whl (811 kB)\n",
            "Installing collected packages: langchain\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.1.4\n",
            "    Uninstalling langchain-0.1.4:\n",
            "      Successfully uninstalled langchain-0.1.4\n",
            "Successfully installed langchain-0.1.6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  WARNING: The script langchain-server.exe is installed in 'C:\\Users\\USER\\AppData\\Roaming\\Python\\Python310\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting langchain-community==0.0.19\n",
            "  Using cached langchain_community-0.0.19-py3-none-any.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in c:\\programdata\\anaconda3\\envs\\project\\lib\\site-packages (from langchain-community==0.0.19) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from langchain-community==0.0.19) (2.0.32)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from langchain-community==0.0.19) (3.10.5)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from langchain-community==0.0.19) (0.6.7)\n",
            "Requirement already satisfied: langchain-core<0.2,>=0.1.21 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from langchain-community==0.0.19) (0.1.23)\n",
            "Requirement already satisfied: langsmith<0.1,>=0.0.83 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from langchain-community==0.0.19) (0.0.87)\n",
            "Requirement already satisfied: numpy<2,>=1 in c:\\programdata\\anaconda3\\envs\\project\\lib\\site-packages (from langchain-community==0.0.19) (1.26.4)\n",
            "Requirement already satisfied: requests<3,>=2 in c:\\programdata\\anaconda3\\envs\\project\\lib\\site-packages (from langchain-community==0.0.19) (2.32.3)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\programdata\\anaconda3\\envs\\project\\lib\\site-packages (from langchain-community==0.0.19) (8.2.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.0.19) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.0.19) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\programdata\\anaconda3\\envs\\project\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.0.19) (23.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.0.19) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.0.19) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.0.19) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.0.19) (4.0.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.0.19) (3.22.0)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.0.19) (0.9.0)\n",
            "Requirement already satisfied: anyio<5,>=3 in c:\\programdata\\anaconda3\\envs\\project\\lib\\site-packages (from langchain-core<0.2,>=0.1.21->langchain-community==0.0.19) (4.2.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from langchain-core<0.2,>=0.1.21->langchain-community==0.0.19) (1.33)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from langchain-core<0.2,>=0.1.21->langchain-community==0.0.19) (23.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from langchain-core<0.2,>=0.1.21->langchain-community==0.0.19) (2.8.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\envs\\project\\lib\\site-packages (from requests<3,>=2->langchain-community==0.0.19) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\envs\\project\\lib\\site-packages (from requests<3,>=2->langchain-community==0.0.19) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\envs\\project\\lib\\site-packages (from requests<3,>=2->langchain-community==0.0.19) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\envs\\project\\lib\\site-packages (from requests<3,>=2->langchain-community==0.0.19) (2024.7.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\programdata\\anaconda3\\envs\\project\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain-community==0.0.19) (4.11.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from SQLAlchemy<3,>=1.4->langchain-community==0.0.19) (3.0.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in c:\\programdata\\anaconda3\\envs\\project\\lib\\site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.21->langchain-community==0.0.19) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\programdata\\anaconda3\\envs\\project\\lib\\site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.21->langchain-community==0.0.19) (1.2.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2,>=0.1.21->langchain-community==0.0.19) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from pydantic<3,>=1->langchain-core<0.2,>=0.1.21->langchain-community==0.0.19) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from pydantic<3,>=1->langchain-core<0.2,>=0.1.21->langchain-community==0.0.19) (2.20.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community==0.0.19) (1.0.0)\n",
            "Using cached langchain_community-0.0.19-py3-none-any.whl (1.6 MB)\n",
            "Installing collected packages: langchain-community\n",
            "  Attempting uninstall: langchain-community\n",
            "    Found existing installation: langchain-community 0.0.20\n",
            "    Uninstalling langchain-community-0.0.20:\n",
            "      Successfully uninstalled langchain-community-0.0.20\n",
            "Successfully installed langchain-community-0.0.19\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-experimental 0.0.64 requires langchain-community<0.3.0,>=0.2.10, but you have langchain-community 0.0.19 which is incompatible.\n",
            "langchain-experimental 0.0.64 requires langchain-core<0.3.0,>=0.2.27, but you have langchain-core 0.1.23 which is incompatible.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: langchain-core==0.1.23 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (0.1.23)\n",
            "Requirement already satisfied: PyYAML>=5.3 in c:\\programdata\\anaconda3\\envs\\project\\lib\\site-packages (from langchain-core==0.1.23) (6.0.1)\n",
            "Requirement already satisfied: anyio<5,>=3 in c:\\programdata\\anaconda3\\envs\\project\\lib\\site-packages (from langchain-core==0.1.23) (4.2.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from langchain-core==0.1.23) (1.33)\n",
            "Requirement already satisfied: langsmith<0.0.88,>=0.0.87 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from langchain-core==0.1.23) (0.0.87)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from langchain-core==0.1.23) (23.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from langchain-core==0.1.23) (2.8.2)\n",
            "Requirement already satisfied: requests<3,>=2 in c:\\programdata\\anaconda3\\envs\\project\\lib\\site-packages (from langchain-core==0.1.23) (2.32.3)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\programdata\\anaconda3\\envs\\project\\lib\\site-packages (from langchain-core==0.1.23) (8.2.3)\n",
            "Requirement already satisfied: idna>=2.8 in c:\\programdata\\anaconda3\\envs\\project\\lib\\site-packages (from anyio<5,>=3->langchain-core==0.1.23) (3.7)\n",
            "Requirement already satisfied: sniffio>=1.1 in c:\\programdata\\anaconda3\\envs\\project\\lib\\site-packages (from anyio<5,>=3->langchain-core==0.1.23) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\programdata\\anaconda3\\envs\\project\\lib\\site-packages (from anyio<5,>=3->langchain-core==0.1.23) (1.2.0)\n",
            "Requirement already satisfied: typing-extensions>=4.1 in c:\\programdata\\anaconda3\\envs\\project\\lib\\site-packages (from anyio<5,>=3->langchain-core==0.1.23) (4.11.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core==0.1.23) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from pydantic<3,>=1->langchain-core==0.1.23) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from pydantic<3,>=1->langchain-core==0.1.23) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\envs\\project\\lib\\site-packages (from requests<3,>=2->langchain-core==0.1.23) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\envs\\project\\lib\\site-packages (from requests<3,>=2->langchain-core==0.1.23) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\envs\\project\\lib\\site-packages (from requests<3,>=2->langchain-core==0.1.23) (2024.7.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain==0.1.6\n",
        "!pip install langchain-community==0.0.19\n",
        "!pip install langchain-core==0.1.23\n",
        "# !pip install langchain-google-genai==0.0.6\n",
        "# !pip install langchain-openai==0.0.2.post1\n",
        "# !pip install docx2txt\n",
        "# !pip install chromadb\n",
        "# !pip install huggingface_hub\n",
        "# !pip install cohere\n",
        "# !pip install streamlit==1.28.0\n",
        "# !pip install --upgrade tiktoken\n",
        "# !pip install Pillow\n",
        "# !pip install numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TflltTQHpid7"
      },
      "outputs": [],
      "source": [
        "!pip install -qU lark chromadb\n",
        "!pip install lark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLGnpWcUpaMb",
        "outputId": "2746515e-c0c2-4515-e1ed-06b755299d9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-4.3.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.12.2)\n",
            "Downloading pypdf-4.3.1-py3-none-any.whl (295 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/295.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-4.3.1\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.2.12-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.32)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.10.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting langchain<0.3.0,>=0.2.13 (from langchain-community)\n",
            "  Downloading langchain-0.2.14-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain-core<0.3.0,>=0.2.30 (from langchain-community)\n",
            "  Downloading langchain_core-0.2.34-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.0 (from langchain-community)\n",
            "  Downloading langsmith-0.1.101-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.26.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.32.3)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain-community)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.3.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain<0.3.0,>=0.2.13->langchain-community)\n",
            "  Downloading langchain_text_splitters-0.2.2-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.13->langchain-community) (2.8.2)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.30->langchain-community)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.30->langchain-community) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.30->langchain-community) (4.12.2)\n",
            "Collecting httpx<1,>=0.23.0 (from langsmith<0.2.0,>=0.1.0->langchain-community)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.0->langchain-community)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2024.7.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community) (3.7.1)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.30->langchain-community)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.13->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.13->langchain-community) (2.20.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community) (1.2.2)\n",
            "Downloading langchain_community-0.2.12-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading langchain-0.2.14-py3-none-any.whl (997 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m997.8/997.8 kB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.2.34-py3-none-any.whl (393 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.9/393.9 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langsmith-0.1.101-py3-none-any.whl (148 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m148.9/148.9 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\n",
            "Downloading marshmallow-3.22.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tenacity, orjson, mypy-extensions, marshmallow, jsonpointer, h11, typing-inspect, jsonpatch, httpcore, httpx, dataclasses-json, langsmith, langchain-core, langchain-text-splitters, langchain, langchain-community\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "Successfully installed dataclasses-json-0.6.7 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.2.14 langchain-community-0.2.12 langchain-core-0.2.34 langchain-text-splitters-0.2.2 langsmith-0.1.101 marshmallow-3.22.0 mypy-extensions-1.0.0 orjson-3.10.7 tenacity-8.5.0 typing-inspect-0.9.0\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.9/14.9 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m83.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m908.3/908.3 kB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.4/431.4 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.7/274.7 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m82.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.8/80.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.7.4)\n",
            "Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.7.0\n",
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.1.22-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.33 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (0.2.34)\n",
            "Collecting openai<2.0.0,>=1.40.0 (from langchain_openai)\n",
            "  Downloading openai-1.42.0-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (0.7.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.33->langchain_openai) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.33->langchain_openai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.33->langchain_openai) (0.1.101)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.33->langchain_openai) (24.1)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.33->langchain_openai) (2.8.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.33->langchain_openai) (8.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.33->langchain_openai) (4.12.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (0.27.0)\n",
            "Collecting jiter<1,>=0.4.0 (from openai<2.0.0,>=1.40.0->langchain_openai)\n",
            "  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (4.66.5)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->langchain_openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->langchain_openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.40.0->langchain_openai) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.40.0->langchain_openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.40.0->langchain_openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.33->langchain_openai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.33->langchain_openai) (3.10.7)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3.0,>=0.2.33->langchain_openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3.0,>=0.2.33->langchain_openai) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (2.0.7)\n",
            "Downloading langchain_openai-0.1.22-py3-none-any.whl (51 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.42.0-py3-none-any.whl (362 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.9/362.9 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jiter, openai, langchain_openai\n",
            "Successfully installed jiter-0.5.0 langchain_openai-0.1.22 openai-1.42.0\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-0.5.5-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.2.1)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.8.2)\n",
            "Collecting chroma-hnswlib==0.7.6 (from chromadb)\n",
            "  Downloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n",
            "Collecting fastapi>=0.95.2 (from chromadb)\n",
            "  Downloading fastapi-0.112.1-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvicorn-0.30.6-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.22.5 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.26.4)\n",
            "Collecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-3.5.2-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.12.2)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.19.0)\n",
            "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.26.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.26.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.47b0-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.26.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.19.1)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.66.5)\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.4.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.64.1)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.2.0-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.12.3)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-30.1.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (8.5.0)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.0.2)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-4.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.10.7)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.27.0)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (24.1)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (1.1.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (2.0.1)\n",
            "Collecting starlette<0.39.0,>=0.37.2 (from fastapi>=0.95.2->chromadb)\n",
            "  Downloading starlette-0.38.2-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.27.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.32.3)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.7)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.2)\n",
            "Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting importlib-metadata<=8.0.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb)\n",
            "  Downloading importlib_metadata-8.0.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.63.2)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.26.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.26.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.26.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.26.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-instrumentation-asgi==0.47b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.47b0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting opentelemetry-instrumentation==0.47b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation-0.47b0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.47b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.47b0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-util-http==0.47b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_util_http-0.47b0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.47b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (71.0.4)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.47b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.16.0)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.47b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (2.20.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb) (0.23.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (13.7.1)\n",
            "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading websockets-13.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2024.6.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=8.0.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.20.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kubernetes>=28.1.0->chromadb) (3.3.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (2.16.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.2.2)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.0)\n",
            "Downloading chromadb-0.5.5-py3-none-any.whl (584 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m584.3/584.3 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.2.0-cp39-abi3-manylinux_2_28_x86_64.whl (273 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.8/273.8 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.112.1-py3-none-any.whl (93 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kubernetes-30.1.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-4.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (67 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.26.0-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.26.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.26.0-py3-none-any.whl (17 kB)\n",
            "Downloading opentelemetry_proto-1.26.0-py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.47b0-py3-none-any.whl (11 kB)\n",
            "Downloading opentelemetry_instrumentation-0.47b0-py3-none-any.whl (29 kB)\n",
            "Downloading opentelemetry_instrumentation_asgi-0.47b0-py3-none-any.whl (15 kB)\n",
            "Downloading opentelemetry_semantic_conventions-0.47b0-py3-none-any.whl (138 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_util_http-0.47b0-py3-none-any.whl (6.9 kB)\n",
            "Downloading opentelemetry_sdk-1.26.0-py3-none-any.whl (109 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.5/109.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading posthog-3.5.2-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.5/41.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.30.6-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading importlib_metadata-8.0.0-py3-none-any.whl (24 kB)\n",
            "Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading starlette-0.38.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvloop-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (427 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m427.7/427.7 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-13.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (157 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.2/157.2 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Building wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53724 sha256=5f3e9f759d4c4fbde841eb03acad2b60598977545cf82ed3eabcb16d0cb1d1d0\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, monotonic, mmh3, websockets, uvloop, uvicorn, python-dotenv, overrides, opentelemetry-util-http, opentelemetry-proto, importlib-metadata, httptools, deprecated, chroma-hnswlib, bcrypt, asgiref, watchfiles, starlette, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, opentelemetry-semantic-conventions, opentelemetry-instrumentation, kubernetes, fastapi, opentelemetry-sdk, opentelemetry-instrumentation-asgi, opentelemetry-instrumentation-fastapi, opentelemetry-exporter-otlp-proto-grpc, chromadb\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib_metadata 8.2.0\n",
            "    Uninstalling importlib_metadata-8.2.0:\n",
            "      Successfully uninstalled importlib_metadata-8.2.0\n",
            "Successfully installed asgiref-3.8.1 bcrypt-4.2.0 chroma-hnswlib-0.7.6 chromadb-0.5.5 deprecated-1.2.14 fastapi-0.112.1 httptools-0.6.1 importlib-metadata-8.0.0 kubernetes-30.1.0 mmh3-4.1.0 monotonic-1.6 opentelemetry-api-1.26.0 opentelemetry-exporter-otlp-proto-common-1.26.0 opentelemetry-exporter-otlp-proto-grpc-1.26.0 opentelemetry-instrumentation-0.47b0 opentelemetry-instrumentation-asgi-0.47b0 opentelemetry-instrumentation-fastapi-0.47b0 opentelemetry-proto-1.26.0 opentelemetry-sdk-1.26.0 opentelemetry-semantic-conventions-0.47b0 opentelemetry-util-http-0.47b0 overrides-7.7.0 posthog-3.5.2 pypika-0.48.9 python-dotenv-1.0.1 starlette-0.38.2 uvicorn-0.30.6 uvloop-0.20.0 watchfiles-0.23.0 websockets-13.0\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.3/204.3 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install pypdf\n",
        "!pip install -U langchain-community\n",
        "!pip install -qU rapidocr-onnxruntime\n",
        "!pip install -qU unstructured\n",
        "!pip install -qU langchain-text-splitters\n",
        "!pip install tiktoken\n",
        "!pip install -U langchain_openai\n",
        "!pip install chromadb\n",
        "!pip install -qU langchain_experimental langchain_openai\n",
        "%pip install -qU langchain langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnv7R-5ypJw6"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "import os\n",
        "from pathlib import Path\n",
        "import openai\n",
        "from langchain_openai import OpenAI, OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.retrievers import ContextualCompressionRetriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "O-Bk8y4UpJw7"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# PDF 문서가 있는 디렉토리\n",
        "PDF_DIR = Path(\"./data\").resolve()\n",
        "\n",
        "# 임시 파일을 저장할 디렉토리\n",
        "TMP_DIR = PDF_DIR.joinpath(\"tmp\")\n",
        "\n",
        "# 벡터 스토어를 저장할 디렉토리\n",
        "LOCAL_VECTOR_STORE_DIR = PDF_DIR.joinpath(\"vector_stores\")\n",
        "\n",
        "# 디렉토리가 존재하지 않으면 생성\n",
        "PDF_DIR.mkdir(parents=True, exist_ok=True)\n",
        "TMP_DIR.mkdir(parents=True, exist_ok=True)\n",
        "LOCAL_VECTOR_STORE_DIR.mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DsE9ocX9pJw8",
        "outputId": "bb8b0d60-ba71-4d3e-d347-af83d93fc4a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI API Key:··········\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "openai_api_key = os.environ['OPENAI_API_KEY'] = getpass.getpass('OpenAI API Key:')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzMa94LzpJw9"
      },
      "source": [
        "## why RAG?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVwZEnYWpJw_",
        "outputId": "2efd7279-ab40-411a-f273-54148ab834af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024년 인공지능 산업은 여러 분야에서 큰 도약을 이룰 것으로 예상됩니다. 몇 가지 중요한 동향을 살펴보겠습니다:\n",
            "\n",
            "1. **생성형 AI의 진화**: 텍스트, 이미지, 음악 등 다양한 형태의 데이터를 생성하는 AI 모델들이 더욱 정교해지고 다양해질 것입니다. 이로 인해 창작, 엔터테인먼트, 마케팅 등에 혁신적인 변화가 있을 것으로 보입니다.\n",
            "\n",
            "2. **산업 자동화**: 제조업, 물류, 농업 등 다양한 산업 분야에서 AI를 이용한 자동화가 크게 확산될 것입니다. 특히 로봇과 AI의 결합으로 기존 공정들을 혁신적으로 개선할 수 있습니다.\n",
            "\n",
            "3. **맞춤형 AI 솔루션**: 의료, 금융, 교육 등 특정 산업에 특화된 AI 솔루션이 더욱 발전할 것입니다. 예를 들어, 개인 맞춤형 치료법을 제안하는 의료 AI나 학생 개개인에 맞춘 학습 계획을 세우는 교육 AI가 이에 해당됩니다.\n",
            "\n",
            "4. **신뢰성과 윤리성 강화**: AI의 활용이 늘어남에 따라 신뢰성과 윤리성이 중요한 이슈로 떠오를 것입니다. AI의 투명성, 설명 가능성, 공정성, 개인정보 보호 등을 보장하기 위한 기술적, 법적, 사회적 노력이 강화될 것입니다.\n",
            "\n",
            "5. **AI와 인간의 협업**: 인간과 AI가 상호 보완하며 협력하는 방식이 발전할 것입니다. 이는 단순히 업무 자동화를 넘어, 인간의 창의성과 AI의 분석력을 결합하여 더 나은 결과를 도출하는 것을 목표로 합니다.\n",
            "\n",
            "6. **엣지 AI**: 클라우드 컴퓨팅의 한계를 넘어서 데이터를 생성하는 현장에서 실시간으로 데이터를 처리할 수 있는 엣지 AI 기술이 중요해질 것입니다. 이는 지연 시간 감소, 보안 강화, 데이터 전송 비용 절감 등의 이점을 제공합니다.\n",
            "\n",
            "7. **환경 및 지속 가능성**: AI가 에너지 사용 최적화, 기후 변화 예측, 재생 가능 에너지 관리 등 환경 보호와 지속 가능성 문제 해결에도 중요한 역할을 할 것입니다.\n",
            "\n",
            "8. **글로벌 경쟁**: 미국, 중국, 유럽 등 주요 국가들이 AI 기술 연구와 개발에 있어 치열한 경쟁을 벌일 것입니다. 이는 국가 차원의 전략적 투자와 정책 결정에도 큰 영향을 미칠 것으로 예상됩니다.\n",
            "\n",
            "위와 같은 동향들은 2024년 AI 산업의 핵심 변화로, 전반적인 기술 발전과 사회적 변화에 큰 영향을 미칠 것입니다.\n"
          ]
        }
      ],
      "source": [
        "from openai import OpenAI\n",
        "try:\n",
        "    client = OpenAI() # defaults to getting the key using os.environ.get(\"OPENAI_API_KEY\")\n",
        "except:\n",
        "    client = OpenAI(api_key=openai_api_key) # if OPENAI_API_KEY is not created as environment variable\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-4o\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": '2024년 인공지능 산업 동향에 대해 설명해 주세요.'},\n",
        "  ]\n",
        ")\n",
        "\n",
        "print(completion.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cK72F1vFpJw_"
      },
      "source": [
        "## Document Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Brwx47cQpJxA"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 8/8 [00:13<00:00,  1.65s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Number of documents: 188\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from langchain.document_loaders import DirectoryLoader, PyPDFLoader\n",
        "\n",
        "def langchain_document_loader(TMP_DIR):\n",
        "\n",
        "    documents = []\n",
        "\n",
        "    # PDF 파일을 로드하기 위한 DirectoryLoader 설정\n",
        "    pdf_loader = DirectoryLoader(\n",
        "        TMP_DIR.as_posix(), glob=\"**/*.pdf\", loader_cls=PyPDFLoader, show_progress=True\n",
        "    )\n",
        "    documents.extend(pdf_loader.load())\n",
        "\n",
        "    return documents\n",
        "\n",
        "  # load documents\n",
        "documents = langchain_document_loader(TMP_DIR)\n",
        "print(f\"\\nNumber of documents: {len(documents)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "IHz6anzQpJxB",
        "outputId": "c8e2bf1d-e1ba-435b-f99d-44b2938570a4"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "**문서[31]**\n",
              "\n",
              "**페이지 내용** (처음 1000자):\n",
              "\n",
              "1. 정책/법제  2. 기업/산업 3. 기술/연구  4. 인력/교육\n",
              "뉴욕타임스 , 저작권 침해로 오픈AI와 마이크로소프트 고소  \n",
              "n뉴욕타임스가 오픈AI와 마이크로소프트를 상대로 자사의 기사를 챗봇 훈련에 무단으로 활용했다며 \n",
              "저작권 침해 소송을 제기\n",
              "n오픈AI는 뉴욕타임스의 소송이 무익하며 AI 학습을 위한 저작물 이용은 공정이용에 해당한다고 \n",
              "주장하는 한편, 뉴욕타임스가 고의로 프롬프트를 조작했을 가능성을 제기KEY Contents\n",
              "£뉴욕타임스 , 오픈AI에 저작권 침해 소송 제기하며 수십억 달러의 손해배상 요구\n",
              "n뉴욕타임스가 2023년 12월 27일 오픈AI와 마이크로소프트 (이하 MS)를 상대로 저작권 침해 소송을 \n",
              "제기했으며 , 미국 주요 언론사가 저작권 문제로 AI 기업을 고소한 사례는 이번이 처음\n",
              "∙뉴욕타임스는 미국 뉴욕 맨해튼 연방지방법원에 소송을 제기하고 자사가 발행한 수백만 건의 기사가 \n",
              "자동화 챗봇 훈련에 무단으로 사용되었다고 주장하며 , 자사 기사를 그대로 출력한 챗GPT 답변 내용을 \n",
              "저작권 침해의 근거로 제시\n",
              "∙뉴욕타임스는 고유하고 가치 있는 저작물의 불법 복제와 사용에 대하여 수십억 달러의 손해배상 \n",
              "책임을 요구하는 한편, 자사의 저작물을 사용한 모든 챗봇 모델과 교육 데이터의 파기를 요구\n",
              "∙소장에 따르면 뉴욕타임스는 2023 년 4월 오픈AI와 MS 측에 저작물 사용에 대한 우려를 제기하고 \n",
              "생성 AI 제품에 대한 상업 계약과 기술적 보호조치를 포함하는 합의를 모색했다가 결렬됨\n",
              "£오픈AI, 뉴욕타임스의 프롬프트 조작 가능성을 제기\n",
              "n오픈AI는 뉴욕타임스가 제기한 소송에 대하여 2024 년 1월 8일 공식 블로그를 통해 무익한 소송\n",
              "이라고 비판하며 AI 훈련에 저작물을 사용하는 것은 저작권법에 따른 공정이용이라고 주장\n",
              "∙오픈AI는 미국의 AP통신, 독일 악셀 스프링거 (Axel Springer) 등의 언론사와 협력 관계를 맺고 상호 \n",
              "이익이 되는 기회를 만들기 위해 노력하고 있다고 설명\n",
              "∙오픈AI는 AI 모델이 방대한 규모의 인간 지 ...\n",
              "\n",
              "**메타데이터:**\n",
              "\n",
              "{'source': '/content/drive/MyDrive/Colab Notebooks/LLM프로젝트/프로젝트2/project/project/data/tmp/SPRi_AI_Brief_2월호.pdf', 'page': 9}"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import random\n",
        "from IPython.display import Markdown\n",
        "\n",
        "# 랜덤으로 문서 선택\n",
        "random_document_id = random.choice(range(len(documents)))\n",
        "\n",
        "# 선택된 문서의 페이지 내용과 메타데이터를 한국어로 출력\n",
        "Markdown(f\"**문서[{random_document_id}]**\\n\\n**페이지 내용** (처음 1000자):\\n\\n\" +\n",
        "         documents[random_document_id].page_content[0:1000] + \" ...\" +\n",
        "         \"\\n\\n**메타데이터:**\\n\\n\" + str(documents[random_document_id].metadata))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSUpLKzrpJxC"
      },
      "source": [
        "## Text Splitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQqoEQ2hpJxC",
        "outputId": "74319576-9fc6-4fc8-b635-4d3f3120da6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of Chunks: 350\n",
            "Number of tokens - Average : 613\n",
            "Number of tokens - 25% percentile : 416\n",
            "Number of tokens - 50% percentile : 596\n",
            "Number of tokens - 75% percentile : 899\n",
            "\n",
            "Max_tokens for gpt-4o: 4096\n"
          ]
        }
      ],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# RecursiveCharacterTextSplitter 생성\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],  # 문서를 나누기 위한 구분자\n",
        "    chunk_size=1000,                     # 각 조각의 최대 크기 (한글 문서에 맞게 조정 가능)\n",
        "    chunk_overlap=100                    # 조각 사이의 중첩 크기 (문맥 일관성 유지)\n",
        ")\n",
        "\n",
        "# 문서 분할\n",
        "chunks = text_splitter.split_documents(documents=documents)\n",
        "\n",
        "# 조각의 수 출력\n",
        "print(f\"Number of Chunks: {len(chunks)}\")\n",
        "\n",
        "import tiktoken\n",
        "\n",
        "def tiktoken_tokens(documents, model=\"gpt-3.5-turbo\"):\n",
        "    \"\"\"OpenAI 모델의 토크나이저(tiktoken)를 사용하여 문서별 토큰 길이 리스트를 반환\"\"\"\n",
        "    encoding = tiktoken.encoding_for_model(model)  # 모델에 사용되는 인코딩을 반환합니다.\n",
        "\n",
        "    tokens_length = [len(encoding.encode(documents[i].page_content)) for i in range(len(documents))]\n",
        "\n",
        "    return tokens_length\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "chunks_length = tiktoken_tokens(chunks,model=\"gpt-3.5-turbo\")\n",
        "\n",
        "print(f\"Number of tokens - Average : {int(np.mean(chunks_length))}\")\n",
        "print(f\"Number of tokens - 25% percentile : {int(np.quantile(chunks_length,0.25))}\")\n",
        "print(f\"Number of tokens - 50% percentile : {int(np.quantile(chunks_length,0.5))}\")\n",
        "print(f\"Number of tokens - 75% percentile : {int(np.quantile(chunks_length,0.75))}\")\n",
        "print(\"\\nMax_tokens for gpt-4o: 4096\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEE-RcFupJxE"
      },
      "source": [
        "## Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-kllr3wpJxE",
        "outputId": "5d8d136f-1f98-4d9c-f4fb-9a4f1436c25b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:141: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ],
      "source": [
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "\n",
        "def select_embeddings_model():\n",
        "\n",
        "    embeddings = OpenAIEmbeddings(\n",
        "        model='text-embedding-3-small',\n",
        "        api_key=openai_api_key\n",
        "    )\n",
        "    return embeddings\n",
        "\n",
        "# OpenAI 임베딩 모델 선택\n",
        "embeddings_OpenAI = select_embeddings_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiS2jWA0pJxF"
      },
      "source": [
        "## Vectorsores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4C-BGYe3pJxF"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "def create_vectorstore(embeddings, documents, vectorstore_name):\n",
        "    \"\"\"Chroma 벡터 데이터베이스를 생성\"\"\"\n",
        "    persist_directory = LOCAL_VECTOR_STORE_DIR.joinpath(vectorstore_name).as_posix()\n",
        "    vector_store = Chroma.from_documents(\n",
        "        documents=documents,\n",
        "        embedding=embeddings,\n",
        "        persist_directory=persist_directory\n",
        "    )\n",
        "    return vector_store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Rv8eSBwpJxG",
        "outputId": "499eca58-cd41-4ea7-b4ea-366c0199d486"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vector_store_OpenAI: 700 chunks.\n",
            "CPU times: user 6.58 s, sys: 194 ms, total: 6.77 s\n",
            "Wall time: 11.9 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "create_vectorstores = True  # True로 설정하여 벡터 스토어를 생성\n",
        "\n",
        "if create_vectorstores:\n",
        "    vector_store_OpenAI = create_vectorstore(\n",
        "        embeddings=embeddings_OpenAI,\n",
        "        documents=chunks,\n",
        "        vectorstore_name=\"Korean_PDF_OpenAI_Embeddings\"\n",
        "    )\n",
        "    print(\"vector_store_OpenAI:\", vector_store_OpenAI._collection.count(), \"chunks.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkyYwkpapJxG"
      },
      "source": [
        "### Load Chroma vectorstore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPbEdV-PpJxG",
        "outputId": "dc4398bf-098d-4018-e5b5-1e776a02f4c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vector_store_OpenAI: 700 chunks.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:141: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 0.4. An updated version of the class exists in the langchain-chroma package and should be used instead. To use it run `pip install -U langchain-chroma` and import as `from langchain_chroma import Chroma`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ],
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "# OpenAI 벡터 스토어 로드\n",
        "vector_store_OpenAI = Chroma(\n",
        "    persist_directory=LOCAL_VECTOR_STORE_DIR.joinpath(\"Korean_PDF_OpenAI_Embeddings\").as_posix(),\n",
        "    embedding_function=embeddings_OpenAI\n",
        ")\n",
        "print(\"vector_store_OpenAI:\", vector_store_OpenAI._collection.count(), \"chunks.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQI_PvH4pJxH"
      },
      "source": [
        "### Similiraty search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdQ5NApqpJxH"
      },
      "outputs": [],
      "source": [
        "# 문서를 출력하는 데에 도움을 주는 함수\n",
        "def print_documents(docs, search_with_score=False):\n",
        "\n",
        "    if search_with_score:\n",
        "        # 유사도 검색 시 점수와 함께 문서를 출력\n",
        "        print(\n",
        "            f\"\\n{'-' * 100}\\n\".join(\n",
        "                [f\"문서 {i+1}:\\n\\n\" + doc[0].page_content + \"\\n\\n점수: \" + str(round(doc[-1], 3)) + \"\\n\"\n",
        "                 for i, doc in enumerate(docs)]\n",
        "            )\n",
        "        )\n",
        "    else:\n",
        "        # 유사도 검색 또는 최대 마진 검색 시 문서만 출력\n",
        "        print(\n",
        "            f\"\\n{'-' * 100}\\n\".join(\n",
        "                [f\"문서 {i+1}:\\n\\n\" + doc.page_content\n",
        "                 for i, doc in enumerate(docs)]\n",
        "            )\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49DqZXwKsZEf",
        "outputId": "40ef8832-5d7a-4e3c-e62e-60805c8fc9ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "문서 1:\n",
            "\n",
            "Ⅰ. 인공지능 산업 동향 브리프\n",
            "\n",
            "점수: 0.732\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "문서 2:\n",
            "\n",
            "Ⅰ. 인공지능 산업 동향 브리프\n",
            "\n",
            "점수: 0.732\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "문서 3:\n",
            "\n",
            "Ⅰ. 인공지능 산업 동향 브리프\n",
            "\n",
            "점수: 0.732\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "문서 4:\n",
            "\n",
            "Ⅰ. 인공지능 산업 동향 브리프\n",
            "\n",
            "점수: 0.732\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 가장 유사한 문서 가져오기 - 점수 포함\n",
        "# 코사인 유사도를 사용, 점수가 낮을수록 좋음\n",
        "\n",
        "query = '2024년 인공지능 산업 동향에 대해 설명해 주세요.'\n",
        "docs_withScores = vector_store_OpenAI.similarity_search_with_score(query, k=4)\n",
        "\n",
        "print_documents(docs_withScores, search_with_score=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9E546chpJxI"
      },
      "source": [
        "### Dot product"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lM_EIbJhpJxJ",
        "outputId": "5744958e-648e-42d3-8672-ca55e92deabf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "문서_1과 쿼리의 유사도: 0.6339\n",
            "문서_2과 쿼리의 유사도: 0.6339\n",
            "문서_3과 쿼리의 유사도: 0.6339\n",
            "문서_4과 쿼리의 유사도: 0.6339\n"
          ]
        }
      ],
      "source": [
        "# 쿼리와 문서의 임베딩을 계산\n",
        "query_embeddings = embeddings_OpenAI.embed_query(query)\n",
        "docs_embeddings = embeddings_OpenAI.embed_documents(\n",
        "    [doc[0].page_content for doc in docs_withScores]\n",
        ")\n",
        "\n",
        "# 각 문서와 쿼리 간의 내적을 계산하여 유사성을 출력\n",
        "# 내적 점수는 점수가 높을수록 쿼리와 문서 간의 유사도가 높음\n",
        "for i in range(len(docs_embeddings)):\n",
        "    dot_product = round(np.dot(query_embeddings, docs_embeddings[i]), 4)\n",
        "    print(f\"문서_{i+1}과 쿼리의 유사도: {dot_product}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTO7UcYHpJxJ"
      },
      "source": [
        "### MMR (max marginal relevance search)\n",
        "max marginal relevance search는 쿼리와 관련된 문서들을 검색할 때, 최대 주변적 중요성 (MMR) 기법을 사용하는 방법 <br>\n",
        "이 방법은 쿼리와 관련된 문서들을 찾을 뿐만 아니라, 선택된 문서들 간의 중복성을 줄여주는 역할도 함"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCRQDglepJxJ",
        "outputId": "8e5d6727-7857-457f-e87a-7d07b928f026"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "문서 1:\n",
            "\n",
            "Ⅰ. 인공지능 산업 동향 브리프\n",
            "----------------------------------------------------------------------------------------------------\n",
            "문서 2:\n",
            "\n",
            "2024 년 2월호\n",
            "Ⅰ. 인공지능 산업 동향 브리프\n",
            " 1. 정책/법제 \n",
            "   ▹ OECD, 금융 분야의 생성 AI 위험과 정책 고려사항을 다룬 보고서 발간························· 1\n",
            "   ▹ 일본 AI 전략회의 , AI의 안전한 활용을 위한 AI 사업자 가이드라인 초안 공개  ··········· 2\n",
            "   ▹ 영국 대법원 , AI는 특허 발명자가 될 수 없다고 최종 판결 ·············································· 3\n",
            "   ▹ 문화체육관광부와 한국저작권위원회 , ‘생성형 AI 저작권 안내서 ’ 발간  ··························· 4\n",
            " \n",
            " 2. 기업/산업 \n",
            "   ▹ 오픈AI, 유료 사용자 대상 GPT 스토어 출시······································································· 5\n",
            "   ▹ 업스테이지 , 콴다와 수학 특화 AI 모델 공동 개발  ·························································· 6\n",
            "   ▹ 뉴욕타임스 , 저작권 침해로 오픈AI와 마이크로소프트 고소   ··········································· 7\n",
            "   ▹ 오픈AI, 생성 AI를 이용한 선거 개입을 막기 위한 대응책 공개  ····································· 8\n",
            "   ▹ CES 2024, 컴패니언 로봇과 일상용품에 이르기까지 다양한 AI 제품 공개 ··················· 9\n",
            " 3. 기술/연구\n",
            "   ▹ 구글 딥마인드 , AI 모델을 활용한 자율로봇 기술 ‘오토RT’ 공개····································· 10\n",
            "   ▹ 구글 딥마인드 , 올림피아드 수준의 기하학 특화 AI ‘알파지오메트리 ’ 공개 ··················· 11\n",
            "----------------------------------------------------------------------------------------------------\n",
            "문서 3:\n",
            "\n",
            "∙지자체 공모를 통해 10개 사업을 우선 선정하여 올해 중 100억원의 예산을 지원하는 한편, 기업들이 AI \n",
            "자율제조 시스템 구축 과정에서 활용할 수 있는 로봇, SW 등의 AI 자율제조 테스트 베드도 구축할 계획\n",
            "n산업부는 또한 업종별 AI 자율제조에 필요한 핵심 역량 확보를 위해 민간 투자를 적극 유치하여 \n",
            "5년간 1조원 이상을 투자할 계획\n",
            "∙정부와 민간의 연구개발 (R&D) 자금은 기계·로봇, 조선, 이차전지 , 반도체 등 주력 제조업의 공정 \n",
            "자동화 , 디지털 트윈 등 가상제조 , 유연 생산 등에 집중 투입할 방침\n",
            "nAI 자율제조 친화형 산업 생태계 조성도 적극 지원하여 , 13,000 명의 전문인력과 250개 이상의 \n",
            "전문기업 (AI 자율제조 솔루션 제공 기업 등)을 육성할 계획\n",
            "∙AI 자율제조 확산을 가로막는 법·제도를 개선하기 위한 작업반도 상반기에 출범하며 , 프라운호퍼 등 \n",
            "선진 연구기관과 국내 연구단체 ·학계 간 업무협약도 체결해 공동 연구개발 , 표준마련 , 실증 등 협력을 \n",
            "강화할 계획 \n",
            "☞ 출처: 산업통상자원부 , 인공지능 (AI) 시대 우리 산업이 나아갈 길을 그린다 , 2024.05.08.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "문서 4:\n",
            "\n",
            "SPRi AI Brief |  2024-6 월호\n",
            "20미국 인구조사국 조사 결과, 2024년 상반기 미국 기업의 AI 사용률은 5.4%\n",
            "n미국 인사조사국에 따르면 미국 기업들의 AI 사용률은 2023 년 9월 3.7% 에서 2024 년 2월 \n",
            "5.4% 로 증가했으며 , 올해 하반기에는 6.6% 까지 늘어날 전망\n",
            "n업종 별로는 IT 부문의 AI 사용률이 18.1% 로 가장 높았으며 , AI를 도입한 기업들은 여타 기업 \n",
            "대비 성장성이 높고 직원 교육이나 신규 업무 프로세스 개발에도 적극적KEY Contents\n",
            "£미국 기업 전반의 AI 사용률은 낮지만 , IT 부문을 중심으로 빠르게 증가\n",
            "n미국 인구조사국의 ‘비즈니스 동향과 전망 조사(Business Trends and Outlook Survey)’ 에 따르면 \n",
            "미국 기업들의 AI 사용률은 상대적으로 낮은 수준이나 일부 업종과 지역을  중심으로 빠르게 증가 추세\n",
            "∙미국 기업들의 상품과 서비스 제작 시 AI 사용률은 2023 년 9월 3.7% 에서 올해 2월에는 5.4% 로 \n",
            "증가했으며 , 하반기에는 6.6% 에 달할 전망\n",
            "∙AI를 도입한 소수의 기업 중 AI로 인해 직원을 해고한 사례는 드물며 , AI를 도입한 기업들은 신규 업무 \n",
            "프로세스 개발이나 직원 대상 기술 교육에 적극적이고 성장성도 높은 것으로 나타남\n",
            "n인구조사국에 따르면 다수의 기업이 아직 AI의 필요성을 느끼지 않아 사용률이 낮으나 , AI의 활용 \n",
            "범위가 확대되면 태도가 변화할 가능성도 있음\n",
            "∙이발소 , 네일샵 , 세탁소 등 소규모 사업자들은 AI가 필요하다고 여기지 않으며 , 다양한 비즈니스 \n",
            "문제에 AI를 적용할 수 있는 방안도 상대적으로 부족\n",
            "n업종별 AI 사용률은 IT 부문이 18.1% 로 가장 높고 건설과 농업 부문은 1.8%에 그쳐 업종 별로 다양하\n",
            "며, 인구조사국은 향후 6개월 동안 IT 부문의 AI 사용률이 21.5% 까지 증가할 것으로 예상\n"
          ]
        }
      ],
      "source": [
        "# 쿼리 정의\n",
        "query = '2024년 인공지능 산업 동향에 대해 설명해 주세요.'\n",
        "\n",
        "# MMR을 사용하여 문서 검색\n",
        "docs_MMR = vector_store_OpenAI.max_marginal_relevance_search(query, k=4)\n",
        "\n",
        "# 검색된 문서 출력\n",
        "print_documents(docs_MMR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6bzzmHupJxK"
      },
      "source": [
        "## Retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUTtMZjIpJxK",
        "outputId": "929b88a0-ebb8-49a1-a7c3-d801041e84a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "문서 1:\n",
            "\n",
            "1. 정책/법제  2. 기업/산업 3. 기술/연구  4. 인력/교육\n",
            "1미국 백악관 , 바이든 대통령의 AI 행정명령에 따른 연방 차원의 대응 조치 종합  \n",
            "n미국 백악관은 2023년 10월 발표된 AI 행정명령에 따라 3개월 간 각 연방 부처와 기관들이 \n",
            "수행한 AI 위험 관리 및 혁신 지원 조치를 종합\n",
            "n연방정부는 AI 개발 기업에게 안전 테스트 결과 공유를 요구하고 , 주요기반시설에  대한 AI \n",
            "위험을 평가하는 한편, AI 혁신을 위한 투자를 확대하고 AI 인력 확보 노력을 강화KEY Contents\n",
            "£미국 연방정부 , AI 행정명령에 따라 3개월간 AI 위험 관리와 혁신 지원\n",
            "n미국 백악관은 2024년 1월 29일 여러 연방 부처와 기관의 책임자들로 구성된 AI 위원회를 소집하고 \n",
            "2023년 10월 발표된 바이든 대통령의 AI 행정명령에 따라 3개월 간 주요 조치를 이행\n",
            "n(AI 안전과 보안을 위한 위험 관리) 주요 연방 부처와 기관들은 AI 위험 완화를 위해 AI 시스템의 안전 \n",
            "테스트 결과 및 클라우드 해외 고객 정보 공유, 주요기반시설 AI 위험 평가 등을 추진\n",
            "∙(안전 테스트 결과 공유) 국방물자생산법에 의거해 강력한 AI 시스템 개발 기업에게 안전 테스트 결과 및 AI \n",
            "시스템 관련 주요 정보를 미국 상무부와 공유하도록 의무화\n",
            "∙(클라우드 해외 고객 정보 신고) 해외 고객에게 AI 모델 훈련용 컴퓨팅 인프라를 제공하는 미국 클라우드 \n",
            "기업에게 외국 고객의 정보를 당국에 신고하도록 하는 규칙의 초안을 제안\n",
            "∙(주요기반시설 AI 위험 평가) 국방부 , 교통부 , 재무부 , 보건복지부 등 9개 부처에서 주요기반시설의 AI 위험 \n",
            "평가를 실시하고 국토안보부에 보고서를 제출\n",
            "n(공익을 위한 AI 혁신) AI 행정명령에 따라 AI 혁신에서 미국의 주도권을 강화하기 위한 AI 투자 확대 \n",
            "및 AI 전문 인력 유치와 양성을 추진 \n",
            "∙(국가AI연구자원 ) 국립과학재단은 연구자에게 컴퓨팅 인프라 , 데이터 , 소프트웨어 , AI 모델과 기타 AI\n",
            "----------------------------------------------------------------------------------------------------\n",
            "문서 2:\n",
            "\n",
            "1. 정책/법제  2. 기업/산업 3. 기술/연구  4. 인력/교육\n",
            "1미국 백악관 , AI 행정명령 이후 180일 간 진행된 AI 조치 발표  \n",
            "n미국 바이든 행정부는 AI 행정명령 발표 이후 180일 동안 연방정부 기관이 수행한 AI 위험 관리와 \n",
            "소비자와 근로자 보호, 혁신 촉진과 인력 양성 조치를 개괄 \n",
            "n연방정부 기관들은 AI로 인한 생화학 무기 위협과 주요기반시설 위협 대응, 근로자 보호 지침 \n",
            "개발, 에너지 관리와 과학 연구에서 AI 활용, 연방정부 내 AI 인력 확충 등을 수행KEY Contents\n",
            "£미국 연방 정부기관 , AI 위험관리와 근로자 ·소비자 보호, 혁신 촉진 등의 조치 이행 \n",
            "n미국 백악관은 2024년 4월 29일 바이든 대통령의 AI 행정명령에 따라 연방정부 차원에서 180일간 \n",
            "진행된 AI 조치를 개괄\n",
            "∙2023년 10월 30일 발표된 AI 행정명령은 연방정부 기관들에게 AI의 안전과 위험 관리, 미국인의 개인정보 \n",
            "보호 및 형평성과 시민권 증진, 소비자와 근로자 보호, 혁신 촉진, AI 인재 양성을 요구\n",
            "n(AI 위험 관리) 연방정부 기관들은 AI로 인한 생화학 무기 위협과 주요기반시설 위협 대응, AI를 이용한  \n",
            "소프트웨어  취약점 완화 등의 조치를 이행\n",
            "∙백악관 과학기술정책국은 생화학 무기에 사용될 수 있는 위험한 생물학적 물질 개발에서 AI 오용을 막기 \n",
            "위해 합성 핵산에 대한 선별 검사 프레임워크를 발표\n",
            "∙국토안보부는 주요 기반시설 소유자와 운영자 대상의 AI 안전과 보안 지침을 개발하고 , 주요 기반시설에서  \n",
            "AI의 안전한 개발과 배포를 보장할 AI안전보안이사회 (AI Safety and Security Board) 를 출범\n",
            "∙국방부와 국토안보부가 주축이 되어 중요한 정부 소프트웨어 시스템에서 보안 취약점을 발견하고 \n",
            "해결하기 위한 AI 도구를 시험  \n",
            "n(근로자와 소비자 보호) AI가 근로자와 소비자 및 시민권에 미치는 위험을 완화하는 조치를 이행\n",
            "----------------------------------------------------------------------------------------------------\n",
            "문서 3:\n",
            "\n",
            "1. 정책/법제  2. 기업/산업 3. 기술/연구  4. 인력/교육\n",
            "3미국 백악관 예산관리국 , 연방정부의 AI 사용에 관한 정책 발표\n",
            "n미국 백악관 예산관리국 (OMB) 은 연방 정부기관에 최고AI책임자 지정과 AI거버넌스 위원회의 \n",
            "설립, AI 활용 시 구체적인 보호조치의 마련을 요구하는 정책을 발표\n",
            "nOMB 는 사회의 시급한 과제 해결을 위한 연방 정부기관의 책임 있는 AI 도입을 장려하는 \n",
            "한편, 정부기관 내 AI 인재 확대와 역량 향상을 위한 계획도 준비KEY Contents\n",
            "£예산관리국 , 연방 정부기관의 안전한 AI 사용을 위해 구체적인 보호조치 마련 요구\n",
            "n미국 백악관 예산관리국 (OMB) 이 바이든 대통령의 AI 행정명령에 의거한 후속조치로 2024년 3월 \n",
            "28일 연방 정부기관의 안전하고 책임 있는 AI 사용을 지원하기 위한 정책*을 발표\n",
            "* Advancing Governance, Innovation, and Risk Management for Agency Use of Artificial Intelligence \n",
            "∙바이든 대통령은 2023 년 10월 30일 AI 행정명령을 통해 OMB 에 연방정부의 AI 사용에 관한 지침 \n",
            "개발을 요구했으며 , OMB 는 행정명령에 따라 이번 정책을 마련 \n",
            "n(AI 거버넌스 강화) AI 위험 관리와 혁신 촉진을 위해 각 연방 정부기관은 60일 내에 최고AI책임자\n",
            "(CAIO) 를 지정하고 AI거버넌스위원회를 설립 필요\n",
            "∙OMB 와 과학기술정책실은 12월부터 최고AI책임자협의회를 정기적으로 개최해 연방정부 전반의 AI \n",
            "사용을 관리하며 , 기관 별 AI거버넌스위원회는 차관급이 의장을 맡아 기관 내 AI 사용을 조정 및 관리\n",
            "n(AI 사용의 위험 해결) 연방 정부기관은 2024 년 12월 1일까지 미국인의 권리나 안전에 영향을 \n",
            "미칠 수 있는 방식의 AI 사용 시 구체적인 보호조치를 마련\n",
            "∙의료, 교육, 고용, 주택 등 광범위한 AI 사용에 적용되는 보호조치는 △AI가 대중에 미치는 영향에\n",
            "----------------------------------------------------------------------------------------------------\n",
            "문서 4:\n",
            "\n",
            "1. 정책/법제  2. 기업/산업 3. 기술/연구  4. 인력/교육\n",
            "3미국 상무부 , 중국과 러시아 겨냥한 첨단 AI 모델 수출 규제 검토 \n",
            "n미국 상무부가 AI를 이용한 사이버 공격이나 생물학 무기 개발을 막기 위해 중국과 러시아 \n",
            "등 적대국에 대하여 첨단 AI 모델의 수출을 제한하는 규제를 검토 중 \n",
            "n상무부는 첨단 AI 모델의 수출 통제 시 AI 행정명령에서 안전 테스트 결과 공유를 요구한 모델 \n",
            "성능을 기준점으로 고려 중으로 , 아직 해당 기준을 넘어선 모델은 출시되지 않음KEY Contents\n",
            "£상무부 , AI를 이용한 적대행위 막기 위해 첨단 AI 모델의 대중국 수출 제한 추진\n",
            "n로이터 통신은 2024 년 5월 9일 복수의 소식통을 인용해 바이든 행정부가 중국과 러시아 등을 \n",
            "대상으로 첨단 AI 모델의 수출 제한을 고려 중이라고 보도\n",
            "∙미국 상무부는 챗GPT와 같은 AI 시스템의 기반이 되는 첨단 AI 모델 중 소프트웨어와 학습 데이터를 \n",
            "공개하지 않는 폐쇄형 모델의 수출 제한을 검토 중\n",
            "∙상무부가 공식 논평을 거부한 가운데 , 중국 대사관은 이러한 시도가 “중국이 단호히 반대하는 \n",
            "전형적인 경제적 강압이자 일방적 괴롭힘이라며 , 자국의 이익을 보호하기 위해 필요한 조치를 취할 \n",
            "것”이라고 논평\n",
            "n미국은 지난 2년간 군사 목적의 AI 기술 개발을 막기 위해 첨단 AI 칩의 대중국 수출을 제한해 \n",
            "왔으며 , 이번 조치를 통해 기존 규제를 보완할 방침\n",
            "∙현재 AI 기업들은 정부의 감독 없이 전 세계에 AI 모델을 제공 중으로 , 미국 정부와 민간 연구자들은 \n",
            "적대국이 AI 모델을 사용해 사이버 공격을 하거나 강력한 생물학 무기를 만들어 낼 가능성을 우려\n",
            "∙실제로 마이크로소프트는 지난 2월 중국, 북한, 러시아 , 이란 정부와 연계된 해킹 그룹이 LLM을 \n",
            "이용해 시도한 해킹을 추적했다고 보고\n",
            "∙국토안보부는 2024 년 국토 위협 평가에서 사이버 공격자들이 AI를 이용해 “더 방대하고 신속하며\n"
          ]
        }
      ],
      "source": [
        "def Vectorstore_backed_retriever(vectorstore, search_type=\"similarity\", k=4, score_threshold=None):\n",
        "    \"\"\"벡터 스토어 기반 리트리버 생성\n",
        "        k: 반환할 문서 수 (기본값: 4)\n",
        "        score_threshold: similarity_score_threshold 검색 시 유사성 점수의 최소 임계값 (기본값=None)\n",
        "    \"\"\"\n",
        "    search_kwargs = {}\n",
        "    if k is not None:\n",
        "        search_kwargs['k'] = k\n",
        "    if score_threshold is not None:\n",
        "        search_kwargs['score_threshold'] = score_threshold\n",
        "\n",
        "    retriever = vectorstore.as_retriever(\n",
        "        search_type=search_type,\n",
        "        search_kwargs=search_kwargs\n",
        "    )\n",
        "    return retriever\n",
        "\n",
        "\n",
        "# 벡터 스토어 기반 리트리버 생성 - 유사도 검색을 기본으로 사용\n",
        "retriever = Vectorstore_backed_retriever(vectorstore=vector_store_OpenAI, search_type=\"similarity\", k=4)\n",
        "\n",
        "# 리트리버를 통해 쿼리에 대한 문서 검색\n",
        "query = '미국 백악관, 바이든 대통령의 AI 행정명령에 따른 연방 차원의 대응 조치 종합에 대해 알려줘'\n",
        "retrieved_docs = retriever.get_relevant_documents(query)\n",
        "\n",
        "# 검색된 문서 출력\n",
        "print_documents(retrieved_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpMv9JW0pJxL"
      },
      "source": [
        "### Contextual Compression Retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTWfrEl8pJxL",
        "outputId": "1ad2ee9a-ad1e-42f6-dccc-581d73db83ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "문서 1:\n",
            "\n",
            "인력/교육\n",
            "1미국 백악관 , 바이든 대통령의 AI 행정명령에 따른 연방 차원의 대응 조치 종합  \n",
            "n미국 백악관은 2023년 10월 발표된 AI 행정명령에 따라 3개월 간 각 연방 부처와 기관들이 \n",
            "수행한 AI 위험 관리 및 혁신 지원 조치를 종합\n",
            "n연방정부는 AI 개발 기업에게 안전 테스트 결과 공유를 요구하고 , 주요기반시설에  대한 AI \n",
            "위험을 평가하는 한편, AI 혁신을 위한 투자를 확대하고 AI 인력 확보 노력을 강화KEY Contents\n",
            "£미국 연방정부 , AI 행정명령에 따라 3개월간 AI 위험 관리와 혁신 지원\n",
            "n미국 백악관은 2024년 1월 29일 여러 연방 부처와 기관의 책임자들로 구성된 AI 위원회를 소집하고 \n",
            "2023년 10월 발표된 바이든 대통령의 AI 행정명령에 따라 3개월 간 주요 조치를 이행\n",
            "n(AI 안전과 보안을 위한 위험 관리) 주요 연방 부처와 기관들은 AI 위험 완화를 위해 AI 시스템의 안전 \n",
            "테스트 결과 및 클라우드 해외 고객 정보 공유, 주요기반시설 AI 위험 평가 등을 추진\n",
            "∙(안전 테스트 결과 공유) 국방물자생산법에 의거해 강력한 AI 시스템 개발 기업에게 안전 테스트 결과 및 AI \n",
            "시스템 관련 주요 정보를 미국 상무부와 공유하도록 의무화\n",
            "∙(클라우드 해외 고객 정보 신고) 해외 고객에게 AI 모델 훈련용 컴퓨팅 인프라를 제공하는 미국 클라우드 \n",
            "기업에게 외국 고객의 정보를 당국에 신고하도록 하는 규칙의 초안을 제안\n",
            "∙(주요기반시설 AI 위험 평가) 국방부 , 교통부 , 재무부 , 보건복지부 등 9개 부처에서 주요기반시설의 AI 위험 \n",
            "평가를 실시하고 국토안보부에 보고서를 제출\n",
            "n(공익을 위한 AI 혁신) AI 행정명령에 따라 AI 혁신에서 미국의 주도권을 강화하기 위한 AI 투자 확대 \n",
            "및 AI 전문 인력 유치와 양성을 추진 \n",
            "∙(국가AI연구자원 ) 국립과학재단은 연구자에게 컴퓨팅 인프라 , 데이터 , 소프트웨어 , AI 모델과 기타 AI\n",
            "----------------------------------------------------------------------------------------------------\n",
            "문서 2:\n",
            "\n",
            "인력/교육\n",
            "3미국 백악관 예산관리국 , 연방정부의 AI 사용에 관한 정책 발표\n",
            "n미국 백악관 예산관리국 (OMB) 은 연방 정부기관에 최고AI책임자 지정과 AI거버넌스 위원회의 \n",
            "설립, AI 활용 시 구체적인 보호조치의 마련을 요구하는 정책을 발표\n",
            "nOMB 는 사회의 시급한 과제 해결을 위한 연방 정부기관의 책임 있는 AI 도입을 장려하는 \n",
            "한편, 정부기관 내 AI 인재 확대와 역량 향상을 위한 계획도 준비KEY Contents\n",
            "£예산관리국 , 연방 정부기관의 안전한 AI 사용을 위해 구체적인 보호조치 마련 요구\n",
            "n미국 백악관 예산관리국 (OMB) 이 바이든 대통령의 AI 행정명령에 의거한 후속조치로 2024년 3월 \n",
            "28일 연방 정부기관의 안전하고 책임 있는 AI 사용을 지원하기 위한 정책*을 발표\n",
            "* Advancing Governance, Innovation, and Risk Management for Agency Use of Artificial Intelligence \n",
            "∙바이든 대통령은 2023 년 10월 30일 AI 행정명령을 통해 OMB 에 연방정부의 AI 사용에 관한 지침 \n",
            "개발을 요구했으며 , OMB 는 행정명령에 따라 이번 정책을 마련 \n",
            "n(AI 거버넌스 강화) AI 위험 관리와 혁신 촉진을 위해 각 연방 정부기관은 60일 내에 최고AI책임자\n",
            "(CAIO) 를 지정하고 AI거버넌스위원회를 설립 필요\n",
            "∙OMB 와 과학기술정책실은 12월부터 최고AI책임자협의회를 정기적으로 개최해 연방정부 전반의 AI \n",
            "사용을 관리하며 , 기관 별 AI거버넌스위원회는 차관급이 의장을 맡아 기관 내 AI 사용을 조정 및 관리\n",
            "n(AI 사용의 위험 해결) 연방 정부기관은 2024 년 12월 1일까지 미국인의 권리나 안전에 영향을 \n",
            "미칠 수 있는 방식의 AI 사용 시 구체적인 보호조치를 마련\n",
            "∙의료, 교육, 고용, 주택 등 광범위한 AI 사용에 적용되는 보호조치는 △AI가 대중에 미치는 영향에\n",
            "----------------------------------------------------------------------------------------------------\n",
            "문서 3:\n",
            "\n",
            "1. 정책/법제  2. 기업/산업 3. 기술/연구  4\n",
            "----------------------------------------------------------------------------------------------------\n",
            "문서 4:\n",
            "\n",
            "인력/교육\n",
            "3미국 상무부 , 중국과 러시아 겨냥한 첨단 AI 모델 수출 규제 검토 \n",
            "n미국 상무부가 AI를 이용한 사이버 공격이나 생물학 무기 개발을 막기 위해 중국과 러시아 \n",
            "등 적대국에 대하여 첨단 AI 모델의 수출을 제한하는 규제를 검토 중 \n",
            "n상무부는 첨단 AI 모델의 수출 통제 시 AI 행정명령에서 안전 테스트 결과 공유를 요구한 모델 \n",
            "성능을 기준점으로 고려 중으로 , 아직 해당 기준을 넘어선 모델은 출시되지 않음KEY Contents\n",
            "£상무부 , AI를 이용한 적대행위 막기 위해 첨단 AI 모델의 대중국 수출 제한 추진\n",
            "n로이터 통신은 2024 년 5월 9일 복수의 소식통을 인용해 바이든 행정부가 중국과 러시아 등을 \n",
            "대상으로 첨단 AI 모델의 수출 제한을 고려 중이라고 보도\n",
            "∙미국 상무부는 챗GPT와 같은 AI 시스템의 기반이 되는 첨단 AI 모델 중 소프트웨어와 학습 데이터를 \n",
            "공개하지 않는 폐쇄형 모델의 수출 제한을 검토 중\n",
            "∙상무부가 공식 논평을 거부한 가운데 , 중국 대사관은 이러한 시도가 “중국이 단호히 반대하는 \n",
            "전형적인 경제적 강압이자 일방적 괴롭힘이라며 , 자국의 이익을 보호하기 위해 필요한 조치를 취할 \n",
            "것”이라고 논평\n",
            "n미국은 지난 2년간 군사 목적의 AI 기술 개발을 막기 위해 첨단 AI 칩의 대중국 수출을 제한해 \n",
            "왔으며 , 이번 조치를 통해 기존 규제를 보완할 방침\n",
            "∙현재 AI 기업들은 정부의 감독 없이 전 세계에 AI 모델을 제공 중으로 , 미국 정부와 민간 연구자들은 \n",
            "적대국이 AI 모델을 사용해 사이버 공격을 하거나 강력한 생물학 무기를 만들어 낼 가능성을 우려\n",
            "∙실제로 마이크로소프트는 지난 2월 중국, 북한, 러시아 , 이란 정부와 연계된 해킹 그룹이 LLM을 \n",
            "이용해 시도한 해킹을 추적했다고 보고\n",
            "∙국토안보부는 2024 년 국토 위협 평가에서 사이버 공격자들이 AI를 이용해 “더 방대하고 신속하며\n",
            "----------------------------------------------------------------------------------------------------\n",
            "문서 5:\n",
            "\n",
            "인력/교육\n",
            "1미국 백악관 , AI 행정명령 이후 180일 간 진행된 AI 조치 발표  \n",
            "n미국 바이든 행정부는 AI 행정명령 발표 이후 180일 동안 연방정부 기관이 수행한 AI 위험 관리와 \n",
            "소비자와 근로자 보호, 혁신 촉진과 인력 양성 조치를 개괄 \n",
            "n연방정부 기관들은 AI로 인한 생화학 무기 위협과 주요기반시설 위협 대응, 근로자 보호 지침 \n",
            "개발, 에너지 관리와 과학 연구에서 AI 활용, 연방정부 내 AI 인력 확충 등을 수행KEY Contents\n",
            "£미국 연방 정부기관 , AI 위험관리와 근로자 ·소비자 보호, 혁신 촉진 등의 조치 이행 \n",
            "n미국 백악관은 2024년 4월 29일 바이든 대통령의 AI 행정명령에 따라 연방정부 차원에서 180일간 \n",
            "진행된 AI 조치를 개괄\n",
            "∙2023년 10월 30일 발표된 AI 행정명령은 연방정부 기관들에게 AI의 안전과 위험 관리, 미국인의 개인정보 \n",
            "보호 및 형평성과 시민권 증진, 소비자와 근로자 보호, 혁신 촉진, AI 인재 양성을 요구\n",
            "n(AI 위험 관리) 연방정부 기관들은 AI로 인한 생화학 무기 위협과 주요기반시설 위협 대응, AI를 이용한  \n",
            "소프트웨어  취약점 완화 등의 조치를 이행\n",
            "∙백악관 과학기술정책국은 생화학 무기에 사용될 수 있는 위험한 생물학적 물질 개발에서 AI 오용을 막기 \n",
            "위해 합성 핵산에 대한 선별 검사 프레임워크를 발표\n",
            "∙국토안보부는 주요 기반시설 소유자와 운영자 대상의 AI 안전과 보안 지침을 개발하고 , 주요 기반시설에서  \n",
            "AI의 안전한 개발과 배포를 보장할 AI안전보안이사회 (AI Safety and Security Board) 를 출범\n",
            "∙국방부와 국토안보부가 주축이 되어 중요한 정부 소프트웨어 시스템에서 보안 취약점을 발견하고 \n",
            "해결하기 위한 AI 도구를 시험  \n",
            "n(근로자와 소비자 보호) AI가 근로자와 소비자 및 시민권에 미치는 위험을 완화하는 조치를 이행\n"
          ]
        }
      ],
      "source": [
        "from langchain.retrievers.document_compressors import DocumentCompressorPipeline\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain_community.document_transformers import EmbeddingsRedundantFilter, LongContextReorder\n",
        "from langchain.retrievers.document_compressors import EmbeddingsFilter\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "\n",
        "def create_compression_retriever(embeddings, base_retriever, chunk_size=500, k=16, similarity_threshold=None):\n",
        "    \"\"\"\n",
        "    ContextualCompressionRetriever 생성\n",
        "    기본 리트리버(base_retriever)를 ContextualCompressionRetriever로 감싸서 사용\n",
        "    여기서 압축기는 Document Compressor Pipeline으로, 문서를 더 작은 조각으로 나누고, 중복 문서를 제거하며,\n",
        "    쿼리와 가장 관련 있는 문서를 필터링하고, 중요한 문서가 목록 상단과 하단에 위치하도록 재정렬\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. 문서를 작은 조각으로 분할\n",
        "    splitter = CharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=0, separator=\". \")\n",
        "\n",
        "    # 2. 중복 문서 제거\n",
        "    redundant_filter = EmbeddingsRedundantFilter(embeddings=embeddings)\n",
        "\n",
        "    # 3. 쿼리와 관련된 문서 필터링\n",
        "    relevant_filter = EmbeddingsFilter(embeddings=embeddings, k=k, similarity_threshold=similarity_threshold)\n",
        "\n",
        "    # 4. 문서 재정렬\n",
        "    reordering = LongContextReorder()\n",
        "\n",
        "    # 5. 압축기 파이프라인과 리트리버 생성\n",
        "    pipeline_compressor = DocumentCompressorPipeline(\n",
        "        transformers=[splitter, redundant_filter, relevant_filter, reordering]\n",
        "    )\n",
        "\n",
        "    compression_retriever = ContextualCompressionRetriever(\n",
        "        base_compressor=pipeline_compressor,\n",
        "        base_retriever=base_retriever\n",
        "    )\n",
        "\n",
        "    return compression_retriever\n",
        "\n",
        "# OpenAI 임베딩과 기본 리트리버 설정을 통해 리트리버 생성\n",
        "base_retriever = Vectorstore_backed_retriever(vectorstore=vector_store_OpenAI, search_type=\"similarity\", k=4)\n",
        "compression_retriever = create_compression_retriever(embeddings=embeddings_OpenAI, base_retriever=base_retriever)\n",
        "\n",
        "# 쿼리에 대한 문서 검색\n",
        "query = '미국 백악관, 바이든 대통령의 AI 행정명령에 따른 연방 차원의 대응 조치 종합에 대해 알려주세요'\n",
        "compressed_docs = compression_retriever.get_relevant_documents(query)\n",
        "\n",
        "# 결과 출력\n",
        "print_documents(compressed_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFkV8ogRpJxM"
      },
      "source": [
        "## Retrieval: put it all together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EynyR61DpJxM"
      },
      "outputs": [],
      "source": [
        "def retrieval_blocks(\n",
        "    create_vectorstore=True,\n",
        "    LLM_service=\"OpenAI\",\n",
        "    vectorstore_name=\"Korean_PDF_OpenAI_Embeddings\",\n",
        "    chunk_size=1600, chunk_overlap=200,\n",
        "    retriever_type=\"Vectorstore_backed_retriever\",\n",
        "    base_retriever_search_type=\"similarity\", base_retriever_k=10, base_retriever_score_threshold=None,\n",
        "    compression_retriever_k=16,\n",
        "):\n",
        "    try:\n",
        "        if create_vectorstore:\n",
        "            documents = langchain_document_loader(TMP_DIR)\n",
        "\n",
        "            text_splitter = RecursiveCharacterTextSplitter(\n",
        "                separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
        "                chunk_size=chunk_size,\n",
        "                chunk_overlap=chunk_overlap\n",
        "            )\n",
        "            chunks = text_splitter.split_documents(documents=documents)\n",
        "\n",
        "            embeddings = select_embeddings_model()\n",
        "\n",
        "            vector_store = create_vectorstore(\n",
        "                embeddings=embeddings,\n",
        "                documents=chunks,\n",
        "                vectorstore_name=vectorstore_name,\n",
        "            )\n",
        "\n",
        "        else:\n",
        "            embeddings = select_embeddings_model()\n",
        "            vector_store = Chroma(\n",
        "                persist_directory=LOCAL_VECTOR_STORE_DIR.as_posix() + \"/\" + vectorstore_name,\n",
        "                embedding_function=embeddings\n",
        "            )\n",
        "\n",
        "        base_retriever = Vectorstore_backed_retriever(\n",
        "            vector_store,\n",
        "            search_type=base_retriever_search_type,\n",
        "            k=base_retriever_k,\n",
        "            score_threshold=base_retriever_score_threshold\n",
        "        )\n",
        "        retriever = None\n",
        "\n",
        "        if retriever_type == \"Vectorstore_backed_retriever\":\n",
        "            retriever = base_retriever\n",
        "\n",
        "        if retriever_type == \"Contextual_compression\":\n",
        "            retriever = create_compression_retriever(\n",
        "                embeddings=embeddings,\n",
        "                base_retriever=base_retriever,\n",
        "                k=compression_retriever_k,\n",
        "            )\n",
        "\n",
        "        print(f\"\\n{retriever_type} 리트리버가 성공적으로 생성되었습니다!\")\n",
        "        print(f\"{LLM_service} 임베딩을 사용하는 벡터 스토어 ({vectorstore_name})에서 {vector_store._collection.count()}개의 청크가 있는 관련 문서를 검색합니다.\")\n",
        "\n",
        "        return retriever\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"오류가 발생했습니다: {e}\")\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGImzvFlt6ds",
        "outputId": "18ea1a8c-5e20-4ad3-ceb5-178293c183fb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 8/8 [00:25<00:00,  3.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "에러 발생: 'bool' object is not callable\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "retriever = retrieval_blocks(chunk_size=1100, chunk_overlap=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKw3yb5spJxN"
      },
      "source": [
        "## Conversational retrieval Chain with memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZhZrFe7pJxN"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "def instantiate_LLM(api_key, model_name=\"gpt4o\", temperature=0, top_p=0.95):\n",
        "    \"\"\"\n",
        "    OpenAI의 LLM을 Langchain에서 사용할 수 있도록 초기화합니다.\n",
        "    \"\"\"\n",
        "    llm = ChatOpenAI(\n",
        "        api_key=api_key,\n",
        "        model=model_name,\n",
        "        temperature=temperature,\n",
        "        model_kwargs={\n",
        "            \"top_p\": top_p\n",
        "        }\n",
        "    )\n",
        "\n",
        "    return llm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsVR7KI9pJxN"
      },
      "source": [
        "### memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUG7ckEApJxT"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationSummaryBufferMemory\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "def create_memory(openai_api_key, model_name='gpt4o', memory_max_token=1024):\n",
        "\n",
        "    # gpt-3.5-turbo 모델에 맞게 ConversationSummaryBufferMemory 생성\n",
        "    memory = ConversationSummaryBufferMemory(\n",
        "        max_token_limit=memory_max_token,\n",
        "        llm=ChatOpenAI(model_name=model_name, openai_api_key=openai_api_key, temperature=0.1),\n",
        "        return_messages=True,\n",
        "        memory_key='chat_history',\n",
        "        output_key=\"answer\",\n",
        "        input_key=\"question\"\n",
        "    )\n",
        "\n",
        "    return memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vj_mHIJJpJxT"
      },
      "source": [
        "### Create a function to instantiate the built-in ConversationalRetrievalChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7G77AM2N4QM5"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "\n",
        "# 한국어로 독립된 질문 생성 템플릿\n",
        "standalone_question_template = \"\"\"다음의 대화 내용과 후속 질문을 참고하여, 후속 질문을 독립된 질문으로 재구성해 주세요.\n",
        "질문은 원래 언어(한국어)로 작성되어야 합니다.\n",
        "\n",
        "대화 기록:\n",
        "{chat_history}\n",
        "\n",
        "후속 질문: {question}\n",
        "\n",
        "독립된 질문:\"\"\"\n",
        "\n",
        "standalone_question_prompt = PromptTemplate(\n",
        "    input_variables=['chat_history', 'question'],\n",
        "    template=standalone_question_template\n",
        ")\n",
        "\n",
        "# 응답 생성을 위한 템플릿 함수\n",
        "def answer_template(language=\"korean\"):\n",
        "    if language == \"korean\":\n",
        "        return \"\"\"다음의 대화 기록과 문서 내용을 바탕으로, 독립된 질문에 대한 답변을 생성해 주세요.\n",
        "질문: {question}\n",
        "대화 기록:\n",
        "{chat_history}\n",
        "문서 내용:\n",
        "{context}\n",
        "\n",
        "답변:\"\"\"\n",
        "    else:\n",
        "        # 영어 기본 템플릿\n",
        "        return \"\"\"Based on the following conversation and document context, generate an answer for the standalone question.\n",
        "Question: {question}\n",
        "Chat History:\n",
        "{chat_history}\n",
        "Document Context:\n",
        "{context}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "def create_memory(openai_api_key, model_name='gpt-3.5-turbo', memory_max_token=1024):\n",
        "    # 주어진 API 키와 모델 이름으로 ConversationSummaryBufferMemory 객체 생성\n",
        "    memory = ConversationSummaryBufferMemory(\n",
        "        max_token_limit=memory_max_token,\n",
        "        llm=ChatOpenAI(model_name=model_name, openai_api_key=openai_api_key, temperature=0.1),\n",
        "        return_messages=True,\n",
        "        memory_key='chat_history',\n",
        "        output_key=\"answer\",\n",
        "        input_key=\"question\"\n",
        "    )\n",
        "\n",
        "    return memory\n",
        "\n",
        "def create_ConversationalRetrievalChain(\n",
        "    llm, condense_question_llm,\n",
        "    retriever,\n",
        "    chain_type='stuff',\n",
        "    language=\"korean\",\n",
        "    model_name='gpt-3.5-turbo',\n",
        "    openai_api_key='YOUR_API_KEY'\n",
        "):\n",
        "    \"\"\"ConversationalRetrievalChain 객체를 생성\n",
        "    우선, 후속 질문과 대화 기록을 LLM에 전달하여 질문을 재구성하고 독립된 질문을 생성\n",
        "    이 질문은 retriever를 통해 관련 문서(컨텍스트)를 검색한 후, 검색된 문서와 독립된 질문, 대화 기록을 LLM에 전달하여 응답을 생성\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. 독립된 질문 생성 템플릿 정의\n",
        "    # 후속 질문과 대화 기록을 `condense_question_llm`에 전달하여 독립된 질문을 생성\n",
        "    standalone_question_prompt = PromptTemplate(\n",
        "        input_variables=['chat_history', 'question'],\n",
        "        template=\"\"\"다음의 대화 내용과 후속 질문을 참고하여, 후속 질문을 독립된 질문으로 재구성해 주세요.\n",
        "질문은 원래 언어(한국어)로 작성되어야 합니다.\n",
        "\n",
        "대화 기록:\n",
        "{chat_history}\n",
        "\n",
        "후속 질문: {question}\n",
        "\n",
        "독립된 질문:\"\"\"\n",
        "    )\n",
        "\n",
        "    # 2. 응답 템플릿 정의\n",
        "    # 독립된 질문 + 대화 기록 + 문서 내용(검색된 문서)을 LLM에 전달하여 응답을 생성\n",
        "    answer_prompt = ChatPromptTemplate.from_template(answer_template(language=language))\n",
        "\n",
        "    # 3. gpt-3.5 모델에 맞는 ConversationSummaryBufferMemory 생성\n",
        "    memory = create_memory(openai_api_key=openai_api_key, model_name=model_name)\n",
        "\n",
        "    # 4. ConversationalRetrievalChain 객체 생성\n",
        "    chain = ConversationalRetrievalChain.from_llm(\n",
        "        condense_question_prompt=standalone_question_prompt,\n",
        "        combine_docs_chain_kwargs={'prompt': answer_prompt},\n",
        "        condense_question_llm=condense_question_llm,\n",
        "        memory=memory,\n",
        "        retriever=retriever,\n",
        "        llm=llm,\n",
        "        chain_type=chain_type,\n",
        "        verbose=False,\n",
        "        return_source_documents=True\n",
        "    )\n",
        "\n",
        "    print(\"Conversational retrieval chain이 성공적으로 생성되었습니다!\")\n",
        "\n",
        "    return chain, memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aAD5NJ56yqGY"
      },
      "outputs": [],
      "source": [
        "result1 = {\n",
        "    'question': '대만의 AI 기본법 초안에 포함된 주요 규제 내용은 무엇인가요?',\n",
        "    'context': '대만 국가과학기술위원회는 AI의 위험 완화를 위한 명확한 규칙을 제시한 AI 기본법 초안을 발표했습니다. 이 초안은 AI 시스템의 안전 기준과 검증 체계 수립, 개인정보보호 강화, AI 자동화로 위협받는 근로자 보호 지침 마련 등을 포함하고 있습니다.'\n",
        "}\n",
        "\n",
        "result2 = {\n",
        "    'question': 'OECD는 금융 분야에서 생성 AI의 도입과 관련하여 어떤 위험을 경고했나요?',\n",
        "    'context': 'OECD는 금융 분야에서 생성 AI의 도입으로 효율성이 높아질 수 있지만, 편견과 차별, 데이터 침해 등의 다양한 위험이 발생할 수 있다고 경고했습니다. 이에 따라 정책 입안자들은 생성 AI 도입에 따른 위험을 완화하기 위한 정책적 대응을 고려해야 한다고 강조했습니다.'\n",
        "}\n",
        "\n",
        "result3 = {\n",
        "    'question': 'EU AI 법이 유럽의회에서 어떤 절차를 거쳐 승인되었나요?',\n",
        "    'context': 'EU AI 법은 회원국들의 만장일치 합의를 얻은 후 유럽의회 핵심 위원회에서도 압도적 표차로 승인되었습니다. 올해 안에 제정될 것으로 예상되며, 향후 2년에 걸쳐 점진적으로 발효될 예정입니다.'\n",
        "}\n",
        "\n",
        "result4 = {\n",
        "    'question': '스태빌리티AI가 발표한 스테이블 디퓨전 3의 주요 특징은 무엇인가요?',\n",
        "    'context': \"스테이블 디퓨전 3'는 이전 버전보다 이미지 품질과 정확도가 대폭 향상되었으며, 새로운 기법인 '확산 트랜스포머'를 통해 고품질의 이미지를 생성할 수 있습니다. 이 모델은 8억에서 80억 개의 매개변수를 사용하여 복잡한 프롬프트도 정확히 이해하고 처리할 수 있습니다.\"\n",
        "}\n",
        "\n",
        "result5 = {\n",
        "    'question': '과기정통부가 개최한 국내 최초 생성 AI 레드팀 챌린지의 목적은 무엇인가요?',\n",
        "    'context': '과기정통부는 생성 AI 모델의 잠재적 위험과 취약점을 식별하기 위해 생성 AI 레드팀 챌린지를 개최했습니다. 이 챌린지를 통해 도출된 위험 요소들은 표준화된 대응체계를 구축하는 데 활용될 예정입니다.'\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "DhvKRyxR1Icu",
        "outputId": "d8fd1425-65e6-4d83-ee98-be71bba70208"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'answer'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-80-2d2629ee6536>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcreate_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresult1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresult1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'answer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m: 'answer'"
          ]
        }
      ],
      "source": [
        "create_memory(input_key=result1['question'], output_key=result1['answer'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9AdDokjpJxU"
      },
      "source": [
        "## pull it all together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOoB8lUBpJxV"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
        "from langchain.memory import ConversationSummaryBufferMemory\n",
        "from langchain.schema import Document\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
        "from operator import itemgetter\n",
        "from langchain.schema import format_document,Document\n",
        "from langchain_core.messages import get_buffer_string\n",
        "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
        "from operator import itemgetter\n",
        "\n",
        "\n",
        "def answer_template(language=\"korean\"):\n",
        "    \"\"\"독립된 질문과 대화 기록 및 문서 컨텍스트를 사용하여 LLM이 답변을 생성합니다.\"\"\"\n",
        "\n",
        "    template = f\"\"\"다음 컨텍스트(<'context'></'context'>)를 사용하여 질문에 답변해 주세요.\n",
        "답변은 반드시 지정된 언어로 작성되어야 합니다.\n",
        "\n",
        "<context>\n",
        "{{chat_history}}\n",
        "\n",
        "{{context}}\n",
        "</context>\n",
        "\n",
        "질문: {{question}}\n",
        "\n",
        "언어: {language}.\n",
        "\"\"\"\n",
        "    return template\n",
        "\n",
        "\n",
        "def _combine_documents(docs, document_prompt, document_separator=\"\\n\\n\"):\n",
        "    doc_strings = [format_document(doc, document_prompt) for doc in docs]\n",
        "    return document_separator.join(doc_strings)\n",
        "\n",
        "\n",
        "def create_memory(openai_api_key, model_name='gpt-3.5-turbo', memory_max_token=1024):\n",
        "    \"\"\"ConversationSummaryBufferMemory 객체 생성\"\"\"\n",
        "    memory = ConversationSummaryBufferMemory(\n",
        "        max_token_limit=memory_max_token,\n",
        "        llm=ChatOpenAI(model_name=model_name, openai_api_key=openai_api_key, temperature=0.1),\n",
        "        return_messages=True,\n",
        "        memory_key='chat_history',\n",
        "        output_key=\"answer\",\n",
        "        input_key=\"question\"\n",
        "    )\n",
        "    return memory\n",
        "\n",
        "\n",
        "def custom_ConversationalRetrievalChain(\n",
        "    llm, condense_question_llm,\n",
        "    retriever,\n",
        "    language=\"korean\",\n",
        "    model_name='gpt-3.5-turbo',\n",
        "    openai_api_key='YOUR_API_KEY'\n",
        "):\n",
        "    \"\"\"ConversationalRetrievalChain을 단계별로 생성하는 함수.\"\"\"\n",
        "    ##############################################################\n",
        "    # Step 1: 독립된 질문을 생성하는 체인\n",
        "    ##############################################################\n",
        "\n",
        "    # 1. 메모리 생성: gpt-3.5에 맞는 ConversationSummaryBufferMemory 생성\n",
        "    memory = create_memory(openai_api_key=openai_api_key, model_name=model_name)\n",
        "\n",
        "    # 2. 메모리 로드: RunnableLambda를 사용하여 메모리 로드\n",
        "    loaded_memory = RunnablePassthrough.assign(\n",
        "        chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"chat_history\"),\n",
        "    )\n",
        "\n",
        "    # 3. 후속 질문과 대화 기록을 LLM에 전달하여 독립된 질문을 생성\n",
        "    condense_question_prompt = PromptTemplate(\n",
        "        input_variables=['chat_history', 'question'],\n",
        "        template=\"\"\"다음의 대화 내용과 후속 질문을 참고하여, 후속 질문을 독립된 질문으로 재구성해 주세요.\n",
        "질문은 원래 언어(한국어)로 작성되어야 합니다.\n",
        "\n",
        "대화 기록:\n",
        "{chat_history}\n",
        "\n",
        "후속 질문: {question}\n",
        "\n",
        "독립된 질문:\"\"\"\n",
        "    )\n",
        "\n",
        "    standalone_question_chain = {\n",
        "        \"standalone_question\": {\n",
        "            \"question\": lambda x: x[\"question\"],\n",
        "            \"chat_history\": lambda x: get_buffer_string(x[\"chat_history\"]),\n",
        "        }\n",
        "        | condense_question_prompt\n",
        "        | condense_question_llm\n",
        "        | StrOutputParser(),\n",
        "    }\n",
        "\n",
        "    # 4. 메모리 로드 및 독립된 질문 체인을 결합\n",
        "    chain_question = loaded_memory | standalone_question_chain\n",
        "\n",
        "    ####################################################################################\n",
        "    #   Step 2: 문서를 검색하고, LLM에 전달하여 응답을 생성\n",
        "    ####################################################################################\n",
        "\n",
        "    # 5. 관련 문서 검색\n",
        "    retrieved_documents = {\n",
        "        \"docs\": itemgetter(\"standalone_question\") | retriever,\n",
        "        \"question\": lambda x: x[\"standalone_question\"],\n",
        "    }\n",
        "\n",
        "    # 6. ['chat_history', 'context', 'question'] 변수를 생성하여 answer_prompt에 전달\n",
        "    DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template=\"{page_content}\")\n",
        "    answer_prompt = ChatPromptTemplate.from_template(answer_template(language=language))\n",
        "    answer_prompt_variables = {\n",
        "        \"context\": lambda x: _combine_documents(docs=x[\"docs\"], document_prompt=DEFAULT_DOCUMENT_PROMPT),\n",
        "        \"question\": itemgetter(\"question\"),\n",
        "        \"chat_history\": itemgetter(\"chat_history\")\n",
        "    }\n",
        "\n",
        "    # 7. 메모리 로드, `answer_prompt` 포맷팅 및 LLM에 전달하여 응답 생성\n",
        "    chain_answer = {\n",
        "        \"answer\": loaded_memory | answer_prompt_variables | answer_prompt | llm,\n",
        "        \"docs\": lambda x: [Document(page_content=doc.page_content, metadata=doc.metadata) for doc in x[\"docs\"]],\n",
        "        \"standalone_question\": lambda x: x[\"question\"]\n",
        "    }\n",
        "\n",
        "    # 8. 최종 체인 생성\n",
        "    conversational_retriever_chain = chain_question | retrieved_documents | chain_answer\n",
        "\n",
        "    print(\"Conversational retriever chain이 성공적으로 생성되었습니다!\")\n",
        "\n",
        "    return conversational_retriever_chain, memory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5JGfyTK-hxv"
      },
      "outputs": [],
      "source": [
        "# 질문\n",
        "questions = [\"2024년 인공지능 산업에서 가장 중요한 트렌드는 무엇인가요?\",\n",
        "             \"이 트렌드가 AI 산업의 다른 분야에 어떤 영향을 미칠까요?\",\n",
        "             \"이 트렌드를 활용하여 실질적인 성과를 달성한 기업의 사례를 설명해 주세요.\"\n",
        "             ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZoQ4epSpJxV"
      },
      "source": [
        "## Invoke ConversationalRetrievalChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZafHaLopJxW",
        "outputId": "f97d11e7-b1db-4cf9-e635-1cadcbe397f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Vectorstore_backed_retriever 리트리버가 성공적으로 생성되었습니다!\n",
            "OpenAI 임베딩을 사용하는 벡터 스토어 (Korean_PDF_OpenAI_Embeddings)에서 700개의 청크가 있는 관련 문서를 검색합니다.\n",
            "Conversational retriever chain이 성공적으로 생성되었습니다!\n"
          ]
        }
      ],
      "source": [
        "# 리트리버와 ConversationalRetrievalChain 인스턴스화:\n",
        "\n",
        "retriever_OpenAI = retrieval_blocks(\n",
        "    create_vectorstore=False,\n",
        "    LLM_service=\"OpenAI\",\n",
        "    vectorstore_name=\"Korean_PDF_OpenAI_Embeddings\",\n",
        "    retriever_type=\"Vectorstore_backed_retriever\",  # 기본 벡터 스토어 리트리버 사용\n",
        "    base_retriever_search_type=\"similarity\",\n",
        "    base_retriever_k=10,\n",
        "    compression_retriever_k=16\n",
        ")\n",
        "\n",
        "chain_openAI, memory_openAI = custom_ConversationalRetrievalChain(\n",
        "    llm=instantiate_LLM(\n",
        "        api_key=openai_api_key,\n",
        "        model_name=\"gpt-3.5-turbo\",\n",
        "        temperature=0.5\n",
        "    ),\n",
        "    condense_question_llm=instantiate_LLM(\n",
        "        api_key=openai_api_key,\n",
        "        model_name=\"gpt-3.5-turbo\",\n",
        "        temperature=0.1\n",
        "    ),\n",
        "    retriever=retriever_OpenAI,\n",
        "    language=\"korean\",\n",
        "    model_name=\"gpt-4o\",\n",
        "    openai_api_key=openai_api_key\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hazp5j_0pJxW",
        "outputId": "f48342d3-0f8f-4686-80a7-f72e80a63b10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question[0]: 2024년 인공지능 산업에서 가장 중요한 트렌드는 무엇인가요?\n",
            "Standalone_question: 2024년 인공지능 산업에서 가장 중요한 트렌드는 무엇이라고 생각하시나요?\n",
            "Answer:\n",
            " 2024년 인공지능 산업에서 가장 중요한 트렌드는 AI의 책임 있는 사용과 안전에 대한 관심이 높아지고 있는 것으로 보입니다. 정부들이 AI 규제를 강화하고 책임 있는 사용을 위한 지침을 발표하며, 기업들도 AI 안전 연구에 주목하고 있습니다. 또한, 다양한 기업들이 AI 모델의 안전 테스트 협력을 위한 MOU를 체결하고, 연구 활동에서 책임 있는 AI 사용에 관한 지침을 발표하는 등 AI의 안전한 활용에 대한 노력이 집중되고 있습니다. 이러한 추세가 계속되면서 AI의 책임 있는 사용이 더욱 강조될 것으로 전망됩니다. \n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Question[1]: 이 트렌드가 AI 산업의 다른 분야에 어떤 영향을 미칠까요?\n",
            "Standalone_question: AI 산업의 다른 분야에서도 AI의 책임 있는 사용과 안전에 대한 관심이 높아지고 있는 것으로 보입니다. 이러한 트렌드가 AI 기술의 발전과 혁신에 어떤 영향을 미칠 것으로 예상하시나요?\n",
            "Answer:\n",
            " AI 기술의 발전과 혁신에 대한 관심이 높아지면서 AI의 책임 있는 사용과 안전에 대한 노력이 집중되고 있습니다. 이러한 트렌드가 계속되면 AI 기술의 발전과 혁신에 긍정적인 영향을 미칠 것으로 예상됩니다. 책임 있는 AI 사용을 위한 지침과 안전한 활용을 위한 노력이 늘어나면서 AI 기술의 발전이 보다 안전하고 투명하게 이루어질 것으로 기대됩니다. 또한, 이러한 노력을 통해 사용자와 사회에 대한 신뢰도 높아지고, 새로운 기술을 도입할 때 발생할 수 있는 위험을 줄일 수 있을 것으로 예상됩니다. 따라서, AI의 책임 있는 사용과 안전에 대한 관심이 높아진다면 AI 기술의 발전과 혁신이 보다 지속 가능하고 효과적으로 이루어질 것으로 전망됩니다. \n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Question[2]: 이 트렌드를 활용하여 실질적인 성과를 달성한 기업의 사례를 설명해 주세요.\n",
            "Standalone_question: AI 산업에서의 책임 있는 사용과 안전에 대한 관심이 높아지고 있는데, 이를 실천한 기업 중에서 어떤 성과를 얻은 사례가 있나요?\n",
            "Answer:\n",
            " 책임 있는 사용과 안전에 대한 관심이 높아지고 있는 AI 산업에서 실천한 기업 중 하나는 오픈AI, 메타, 구글, 앤스로픽 등이 있습니다. 이들은 AI 안전성과 신뢰성 연구를 위한 기본적인 보호장치와 공평한 접근을 요구하고 있습니다. 특히 AI 기업들은 취약점 공개 규칙을 확립하여 외부 연구자가 수행하는 AI 안전성, 보안성, 신뢰성 연구에 법적 책임을 면제하고 외부 검토자에게 연구자들의 평가 신청을 관리하도록 하는 등 책임 있는 사용과 안전에 대한 노력을 기울이고 있습니다. 이러한 노력을 통해 사용자와 사회에 대한 신뢰도가 높아지고, AI 기술의 발전과 혁신이 안전하게 이루어질 것으로 기대됩니다. \n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "CPU times: user 1.77 s, sys: 128 ms, total: 1.89 s\n",
            "Wall time: 22 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "memory_openAI.clear()\n",
        "responses = []\n",
        "for i,question in enumerate(questions):\n",
        "    response = chain_openAI.invoke({\"question\":question})\n",
        "    responses.append(response)\n",
        "\n",
        "    answer = response['answer'].content\n",
        "    print(f\"Question[{i}]:\",question)\n",
        "    print(\"Standalone_question:\",response['standalone_question'])\n",
        "    print(\"Answer:\\n\",answer,f\"\\n\\n{'-' * 100}\\n\")\n",
        "\n",
        "    memory_openAI.save_context( {\"question\": question}, {\"answer\": answer} ) # update memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hHjQ0-NkpJxW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  WARNING: The script langsmith.exe is installed in 'C:\\Users\\USER\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "  WARNING: The script langchain-server.exe is installed in 'C:\\Users\\USER\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-google-genai 0.0.6 requires langchain-core<0.2,>=0.1, but you have langchain-core 0.2.34 which is incompatible.\n"
          ]
        }
      ],
      "source": [
        "!pip install -qU langchain_experimental langchain_openai"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "openai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
